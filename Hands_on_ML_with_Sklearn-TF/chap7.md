# 第七章：集成学习与随机森林

假设你向数千人询问一个复杂的问题，然后把他们的答案合并起来。在很多时候，你会发现这个合并的答案要优于专家的答案。这被称为**群众智慧**（*wisdom of the crowd*）。类似地，如果你把一组预测器（比如分类器或回归器）的预测结果合并起来，你通常会得到比最佳单一预测器更好的预测结果。这一组预测器就称为集成，因此，这种技术被称为**集成学习**（*Ensemble Learning*），集成学习算法被称为**集成方法**（*Ensemble method*）。

例如，你能在训练集的不同的随机子集上训练一组决策树分类器。为了进行预测，你得到了所有树的预测结果，将票数最多的类作为预测结果（见第六章最后的练习）。这样一种决策树的集成被称为**随机森林**（*Random Forest*），它尽管很简单，却是当今最强大的机器学习算法之一。

此外，我们在第二章讨论过，在一个项目的最后你会经常使用集成方法，一旦你建立了一些不错的预测器，将它们组合为一个更好的预测器。事实上，机器学习竞赛中的获胜算法经常包含了一些集成方法（最知名的在 [Netflix Prize competition](http://netflixprize.com/)）。

在本章中，我们会讨论最流行的集成方法，包括 bagging， boosting， stacking 还有一些其他的算法。我们也会探索随机森林。

## 投票分类器

假设你已经训练了一些分类器，每个都有 80% 的精度。你可能有一个逻辑回归分类器， SVM 分类器，随机森林分类器，K 近邻分类器等等（见图 7-1 ）。

![1](./images/chap7/7-1.png)

一种创建更优分类器的简单方法是合并每一个分类器，预测类别为票数最多的类。这种多数票分类器被称为硬投票分类器（见图 7-2 ）。

![2](./images/chap7/7-2.png)

令人惊讶的是，这个投票分类器通常会比集成学习中最好的分类器有更高的精度。事实上，即便每个学习器都是**弱学习器**（*weak learner*）（意味着它只比胡乱猜测好一点），集成仍然是一个**强分类器**（*strong learner*）（精度更高），假如弱学习器的数量足够多，它们就会足够多样性。

这怎么可能？下面的类比能帮你解开谜团。假设你有一个有偏差的硬币，有 51% 的概率是正面， 49% 的概率是背面。如果你抛 1000 次硬币，大约会得到 510 次正面和 490 次背面，因此大多数是正面。如果你用数学计算，你会发现在 1000 次抛硬币后获得多数票为正面的概率接近 75% 。你抛硬币的次数越多，这个概率就会越高（例如，你抛 10000 次硬币，概率会升到 97% ）。这是因为**大数定律**（*law of large numbers*）：随着你持续抛硬币，正面的比例会不断接近抛出正面的概率（ 51% ）。图 7-3 展示了 10 种有偏差的硬币抛掷。你能看到，随着抛硬币次数增加，正面的比例越来越接近 51% 。最终这十种都很接近 51% ，被认为大于 50% 。

![3](./images/chap7/7-3.png)

类似地，假设你创建了一个集成，拥有 1000 个单独运行时正确率只有 51% 的分类器（只比胡乱猜测好一点）。如果你预测多数投票类，精度可能会提高到 75% ！不过，这只有当所有的分类器都完全独立、误差互不关联时才成立，显然本例中不可能实现，因为它们是在相同的数据集上训练的。它们很可能会犯同一种错，所以多数票可能会投给错误的类别，从而降低集成的精度。

