# 第七章：集成学习与随机森林

假设你向数千人询问一个复杂的问题，然后把他们的答案合并起来。在很多时候，你会发现这个合并的答案要优于专家的答案。这被称为**群众智慧**（*wisdom of the crowd*）。类似地，如果你把一组预测器（比如分类器或回归器）的预测结果合并起来，你通常会得到比最佳单一预测器更好的预测结果。这一组预测器就称为集成，因此，这种技术被称为**集成学习**（*Ensemble Learning*），集成学习算法被称为**集成方法**（*Ensemble method*）。

例如，你能在训练集的不同的随机子集上训练一组决策树分类器。为了进行预测，你得到了所有树的预测结果，将票数最多的类作为预测结果（见第六章最后的练习）。这样一种决策树的集成被称为**随机森林**（*Random Forest*），它尽管很简单，却是当今最强大的机器学习算法之一。

此外，我们在第二章讨论过，在一个项目的最后你会经常使用集成方法，一旦你建立了一些不错的预测器，将它们组合为一个更好的预测器。事实上，机器学习竞赛中的获胜算法经常包含了一些集成方法（最知名的在 [Netflix Prize competition](http://netflixprize.com/)）。

在本章中，我们会讨论最流行的集成方法，包括 bagging， boosting， stacking 还有一起其他的算法。我们也会探索随机森林。

## 投票分类器

