# 第十六章：强化学习

**强化学习**（*Reinforcement	Learning*，RL）是当今机器学习中最令人兴奋的领域之一，也是最古老的领域之一。它在 20 世纪 50 年代就开始了，多年来产生了许多有趣的应用，尤其是在游戏方面（比如， *TD-Gammon* ，一种双陆棋游戏程序）和机器控制方面，不过很少有能上头条新闻的。但是， 在 2013 年发生了一场革命，来自英国新兴公司 DeepMind 的研究者 [演示了一个能够游玩任何雅达利游戏的系统](https://goo.gl/hceDs5) ，甚至能 [超过大多数人类](https://goo.gl/hgpvz7) ，它只用了很少的像素作为输入，对游戏规则也一无所知。这是一系列令人惊叹的壮举中的第一次，在 2016 年 3 月以 AlphaGo 击败世界围棋冠军李世石而达到高潮。从未有程序能击败围棋领域的大师，更别说是世界冠军了。如今，强化学习的整个领域正在酝酿着新的想法，具有广泛的应用范围。DeepMind 在 2014 年已被谷歌以超过 5 亿的价格收购。

所以他们是怎么办到的？事后看来，它看起来相当简单：他们将深度学习的力量应用于强化学习的领域，而它超越了他们最狂野的梦想。本章中，我们首先会解释强化学习是什么，它擅长什么，之后我们会展示深度强化学习中两种最重要的技术：**策略梯度**（*policy gradients*）和**深度Q-网络**（*Q-networks*，DQN），包括**马尔可夫决策过程**（*Markov decision processes*，MDP）的讨论。我们会使用这些技术来训练一个模型来平衡移动推车上的杆子，另一个模型来游玩雅达利游戏。同样的技术也能用于各种各样的任务，从步行机器人到自动驾驶汽车都可以。


## 学会优化奖赏

在强化学习中，环境（*environment*）中软件的智能体（*agent*）会进行观察（*observations*），做出决策（*actions*），作为回报它会得到奖赏（*rewards*）。它的目标是学习能最大化期望长期奖赏的决策方法。如果你不介意拟人化，你可以把正奖赏看做是快乐，负奖赏看做是痛苦（这时 “奖赏” 一词就有点误导了）。简而言之，智能体在环境中做出决策，通过试错法最大化快乐，最小化痛苦。

这是很广泛的设置，能应用于各种各样的任务。下面是一些例子（见图 16-1 ）：

1. 智能体可以是控制步行机械狗的程序。在本例中，环境是现实世界，智能体通过一组传感器（比如相机和触觉传感器）观察环境，它通过发送信号、启动马达来进行决策。它可以被编程为到达目标位置的时候获得正奖赏，也可以是当它浪费时间、去了错误的位置、跌倒时获得负奖赏。
2. 智能体可以是控制 Ms. Pac-Man 的程序。在本例中，环境是雅达利游戏的模拟，决策是九种可能的操纵杆位置（上左，下，中，等等），观察是摄像机镜头，奖赏就是游戏点数。
3. 类似地，智能体可以是游玩棋类游戏（比如围棋）的程序。
4. 智能体可以不用去控制移动的实体（或虚拟物体）。例如，它可以是一个智能恒温器，当它接近目标温度并节约能量时得到奖赏，当需要人去调节温度时得到负奖赏，所以智能体必须学习预见人类的需求。
5. 智能体可以观察股票市场价格，决定每秒的买进和抛出。奖赏显然是赚钱或赔钱。

![1](./images/chap16/16-1.png)

注意，也有可能完全没有正奖赏。例如，智能体可能在迷宫中移动，每次移动都得到负奖赏，所以要尽可能快的找到出口！还有很多其他适合强化学习的例子，比如自动驾驶汽车，在网页上投放广告，或者控制图像分类系统的关注内容。

## 策略搜索

智能体用于决策的算法被称为策略（*policy*）。例如，策略可以是以观测为输入并输出行为的神经网络（见图 16-2 ）。

![2](./images/chap16/16-2.png)

策略可以是任何你能想到的算法，它甚至可以是不确定的。例如，考虑一个真空吸尘器，奖赏是 30 分钟内清理的灰尘数量。它的策略可以是每秒以概率 p 向前移动，或者以概率 1-p 随机向左向右旋转。旋转角度可以在 -r 到 +r 之间。因为这个策略涉及随机性，它被称为**随机策略**（*stochastic	policy*）。机器会有不规则的轨迹，保证它能达到任何位置，并清理所有灰尘。问：它在 30 分钟能内清理掉多少灰尘？

你该如何训练这样一个机器？只有两个你能调节的策略参数：概率 p 和角度范围 r 。一种可能的学习算法是给参数尝试许多不同的值，挑选表现最好的组合（见图 16-3 ）。这是策略搜索的一个例子，使用了暴力搜索方法。然而，当策略空间过大时（通常是这样），通过这种方式来找到一组优秀的参数宛如大海捞针。

另一种方法是使用**遗传算法**（*genetic algorithms*）探索策略空间。例如，你可以随机创建一个包含 100 个策略的第一代基因， “杀掉” 80 个最糟糕的策略，用幸存的 20 个基因繁衍 4 代。后代只是父辈的备份加上一些随机变种。幸存的基因加上它们的后代组成第二代基因。你可以通过这种方法持续迭代遗传，直到找到一种优秀的策略。

![3](./images/chap16/16-3.png)

还有一种方法是使用优化技术，计算奖赏关于策略参数的梯度，让梯度跟随更大的奖赏（梯度上升），来微调参数。这种方法被称为**策略梯度**（*policy gradients*，PG），我们稍后会在本章讨论具体细节。
