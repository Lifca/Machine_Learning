# 第六章：决策树

就像 SVM ，决策树也是一种多功能机器学习算法，既可以实现分类任务，又能实现回归任务，甚至还能处理多输出任务。它们是很强大的算法，可以拟合复杂的数据集。例如，在第二章中，你在加利福尼亚房价数据集上训练了`DecisionTreeRegressor`模型，拟合效果很好（实际上都过拟合了）。

决策树也是随机森林的基本组成（见第七章），而随机森林是当今最强大的机器学习算法之一。

在本章中，我们会首先讨论如何用决策树训练、可视化以及进行预测。然后我们会使用 Scikit-Learn 来学习 CART 训练算法，讨论如何正则化树并将它们用于回归任务。最后，我们会讨论一些决策树的局限。

## 训练和可视化决策树

为了理解决策树，首先创建一个决策树，看看它是如何进行预测的。下面的代码在鸢尾花数据集（见第四章）上训练了`DecisionTreeClassifier`：

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:, 2:] # petal length and width 
y = iris.target
   
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y)
```

你可以可视化训练好的决策树，通过`export_graphviz()`方法来输出图像定义文件，命名为*iris_tree.dot*。

```python
from sklearn.tree import export_graphviz

export_graphviz(
	tree_clf,
	out_file=image_path("iris_tree.dot"),
	feature_names=iris.feature_names[2:],
	class_names=iris.target_names,
	rounded=True,
	filled=True
	)
```

然后你可以使用`graphviz`包里的`dot`命令行工具，将 *.dot* 文件转换为 PDF 或 PNG 等多种格式。下面的命令行会将 .dot 文件转换为 .png 图像文件：

```
$ dot -Tpng iris_tree.dot -o iris_tree.png
```

第一个决策树如图 6-1 。

![1](./images/chap6/6-1.png)

## 进行预测

来看看图 6-1 中的决策树是如何进行预测的。假设你发现了一朵鸢尾花，想要将它分类。你从根节点（*root node*）（深度为 0 ，在顶端）开始：该节点询问花瓣长度是否小于 2.45 厘米。如果是，就移动到根的左孩子节点（深度为 1，在左边）。在本例中，它是一个叶子节点（即没有孩子节点），所以它不再继续询问：你可以直接查看该节点的预测类别，决策树预测花的类别为山鸢尾（`class=setosa`）。

现在假设你发现了另一朵花，这次的花瓣长度超过了 2.45 厘米。你必须移动到根的右孩子节点（深度为 1 ，在右边），它不是叶子节点，所以它还有一次询问：花瓣宽度是否小于 1.75 厘米？如果是，那么这朵花可能是变色鸢尾（深度为 2 ，在左边）。如果不是，那么它有可能是维吉尼亚鸢尾（深度为 2 ，在右边）。真的很简单。

> **笔记**
> 决策树的众多特性之一就是它们无需太多数据预处理。特别是，它们完全不需要特征缩放或中心化。

节点的`samples`属性统计它应用于多少训练实例。例如，100 个训练实例的花瓣长度都大于 2.45 厘米（深度为 1 ，在右边），其中有 54 个的花瓣宽度小于 1.75 厘米（深度为 2 ，在左边）。节点的`value`属性告诉你该节点的每个类有多少训练实例：例如，右下角的节点有 0 个山鸢尾，1 个 变色鸢尾，45 个维吉尼亚鸢尾。最后，一个节点的`gini`属性测量它的**不纯度**（*impurity*）：如果它应用的所有训练实例都属于同一类，那么就说这个节点是“纯的”（`gini=0`）。例如，因为深度为 1 的左孩子节点只包含了山鸢尾训练实例，所以它是纯的，`gini`数为 0 。公式 6-1 展示了训练算法计算第 ![](http://latex.codecogs.com/gif.latex?%5Cinline%20i) 个节点的基尼指数 ![](http://latex.codecogs.com/gif.latex?%5Cinline%20G_i) 的过程。例如，深度为 2 的左孩子节点的基尼指数为 ![](http://latex.codecogs.com/gif.latex?%5Cinline%201-%280/54%29%5E2-%2849/54%29%5E2-%285/54%29%5E2%5Capprox%200.168) 。另一种不纯度测量方法将会在稍后讨论。

![](http://latex.codecogs.com/gif.latex?G_i%3D1-%5Csum_%7Bk%3D1%7D%5E%7Bn%7Dp_%7Bi%2Ck%7D%5E2)

-  ![](http://latex.codecogs.com/gif.latex?p_%7Bi%2Ck%7D) 是第 ![](http://latex.codecogs.com/gif.latex?%5Cinline%20i) 个节点中训练实例类别为 ![](http://latex.codecogs.com/gif.latex?k) 的比率。

> **笔记**
> Scikit-Learn 使用 CART 算法，只生成二叉树：非叶节点总是有两个孩子节点（即只有“是/否”的回答）。不过，其他的算法——比如 ID3 ，就能生成拥有两个以上节点的决策树。

图 6-2 展示了决策树的决策边界。细的垂直线段代表根节点（深度为 0 ）的决策边界：花瓣长度为 2.45 厘米。因为左区域是纯的（只有山鸢尾），所以它不能再被划分。不过，右区域是不纯的，所以深度为 1 的右节点在花瓣宽度为 1.75 厘米处进行了划分（用虚线表示）。因为`max_depth`被设置为 2 ，决策树就在这里停止了。不过，如果你把它设置为 3 ，那么两个深度为 2 的节点就都会增加一条决策边界（用虚线表示）。

![2](./images/chap6/6-2.png)

> **模型说明：白盒 vs 黑盒**
>
> 如你所见，决策树相当直观，它们的决策都很容易解释。这样的模型通常被称为**白盒模型**（*white box models*）。相反地，随机森林或神经网络通常被认为是**黑盒模型**（*black box models*）。它们能做出很好的预测，你也能很容易检查用于预测的计算过程，尽管如此，通常很难用简明的术语解释做出这样预测的理由。例如，如果一个神经网络说一个特定的人出现在某张照片中，我们很难知道是什么导致了这样的预测：是因为模型认出了此人的眼睛？还是她的嘴？她的鼻子？她的鞋？或者是她坐的沙发？相反地，决策树就提供了良好而简单的分类规则，甚至可以根据需要进行手动应用（比如花的分类）。

## 估计分类概率

决策树也能估计实例属于类别 ![](http://latex.codecogs.com/gif.latex?k) 的概率：首先遍历树来寻找该实例的叶节点，之后返回该节点中类别为 ![](http://latex.codecogs.com/gif.latex?k) 的训练实例的比例。例如，假设你发现了一朵花瓣长 5 厘米，宽 1.5 厘米的花。对应的叶节点是深度为 2 的左孩子节点，所以决策树应该输出下面的概率：山鸢尾—— 0 %（0/54），变色鸢尾—— 90.7 %（49/54），维吉尼亚鸢尾—— 9.3 %（5/54）。当然，如果你要求它预测类别，它会输出变色鸢尾（类别 1 ），因为它的概率最高。来检查一下：

```python
>>> tree_clf.predict_proba([[5, 1.5]])
array([[ 0. , 0.90740741, 0.09259259]])
>>> tree_clf.predict([[5, 1.5]])
array([1])
```

完美！注意，估计概率在任何地方都是相同的，除了图 6-2 右下角的矩形——例如，如果花瓣长 6 厘米，宽 1.5 厘米（尽管在本例中它看起来显然最可能是维吉尼亚鸢尾）。

## CART 训练算法

Scikit-Learn 使用**分类回归树**（*Classification And Regression Tree*）算法来训练决策树（也称为“增长”树）。思路相当简单：算法首先使用单特征 ![](http://latex.codecogs.com/gif.latex?k) 和阈值 ![](http://latex.codecogs.com/gif.latex?t_k) （比如，“花瓣长度小于等于 2.45 厘米”），把训练集划分为两个子集。它是如何选择 ![](http://latex.codecogs.com/gif.latex?k) 和 ![](http://latex.codecogs.com/gif.latex?t_k) 的？它搜索能生成最纯子集的（ ![](http://latex.codecogs.com/gif.latex?k) ， ![](http://latex.codecogs.com/gif.latex?t_k) ）对。算法要最小化的损失函数在公式 6-2 中给出。

![](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_6/104)

一旦它成功把训练集一分为二，它就会使用相同的逻辑再次划分子集，然后是子集的子集，以此类推递归下去。当它到达最大深度（由超参数`max_depth`定义），或是找不到能降低不纯度的划分时，递归就会停止。其他的一些超参数（稍后讨论）控制额外的停止条件（`min_samples_split`，`min_samples_leaf`，`min_weight_fraction_leaf`和`max_leaf_nodes`）。

> **警告**
> 如你所见， CART 算法是一个贪心算法（*greedy algorithm*）：它从顶部开始贪婪地寻找一种最优划分，然后在每一层重复这个过程。它不检查在下降若干层后，划分能否使不纯度达到最低。贪心算法经常能生成一个不错的解决方法，但是并不保证是最优的。

不幸的是，寻找最优树的过程是一个 NP 完全问题（*NP-Complete*）：它的时间复杂度为 ![](http://latex.codecogs.com/gif.latex?O%28%5Cexp%28m%29%29) ，即便训练集很小，问题也会变得难以处理。这就是我们必须找到一种“良好”解决方法的原因。

## 计算复杂度

进行预测需要从根节点到叶节点遍历决策树。决策树通常是近似平衡的，所以遍历决策树大约要经过 ![](http://latex.codecogs.com/gif.latex?O%28%5Clog_2%28m%29%29) 个节点。因为每个节点只需检查一个特征的值，整体预测复杂度仅为 ![](http://latex.codecogs.com/gif.latex?O%28%5Clog_2%28m%29%29) ，与特征数量无关。所以预测速度非常快，即便是处理大型数据集。

不过，训练算法会在每个节点上比较所有特征（如果设置了`max_features`，可能会少一些），导致训练复杂度为 ![](http://latex.codecogs.com/gif.latex?O%28n%5Ctimes%20m%5Clog%28m%29%29) 。对于小型数据集（实例小于几千）， Scikit-Learn 可以通过重排数据（设置`presort=True`）提高训练速度，但是对于大型数据集而言会大幅降低训练速度。

## 基尼不纯度还是信息熵？

测量时默认会使用基尼不纯度，不过你也可以将`criterion`超参数设置为`entropy`，选择熵（*entropy*）的不纯度。熵的概念源于热力学，作为分子混乱度的衡量：当分子静止有序时熵接近零。不久后，熵就扩展到更广泛的领域，包括香农的**信息论**（*information theory*），它测量信息中的平均信息密度：当所有的信息都相同时熵为零。在机器学习中，它被频繁用于不纯度测量：当一个集合只包含一类实例时它的熵为零。公式 6-3 展示了第 ![](http://latex.codecogs.com/gif.latex?i) 个节点上熵的定义。例如，图 6-1 中深度为 2 的左孩子节点的熵等于 ![](http://latex.codecogs.com/gif.latex?-%5Cfrac%7B49%7D%7B54%7D%5Clog%28%5Cfrac%7B49%7D%7B54%7D%29-%5Cfrac%7B5%7D%7B54%7D%5Clog%5Cfrac%7B5%7D%7B54%7D%5Capprox%200.31) 。

![](http://latex.codecogs.com/gif.latex?H_i%3D-%5Csum_%7Bk%3D1%7D%5Enp_%7Bi%2Ck%7D%5Clog%28p_%7Bi%2Ck%7D%29)

所以你该用基尼不纯度还是信息熵？事实上，大多数时候并没有太大的区别：它们会生成类似的树。基尼不纯度的计算要稍微快一些，所以它是个不错的默认选择。不过，当它们产生差异的时候，基尼不纯度偏向于在树的分支中隔离出现最频繁的类别，而信息熵趋向于生成更平衡的树。

## 正则化参数

决策树几乎不对训练数据进行假设（和线性模型完全相反，后者显然假设了数据是线性的）。如果不加约束，树的结构会自己适应训练数据，使自己非常接近地拟合数据，甚至可能会过拟合。诸如此类的模型常被称为**非参数模型**（*nonparametric model*），并不是因为不含任何参数（它通常会有很多参数），而是因为在训练之前没有设定参数数量，所以模型结构能自由根据数据调整自身。相反地，像线性模型那样的**参数模型**（*parametric model*）有确定的参数数量，所以它的自由度是受限的，减少了过拟合的风险（但是增加了欠拟合的风险）。

为了避免过拟合训练数据，你需要限制决策树在训练时的自由度。你现在已经知道了，这个过程叫做正则化。正则化超参数依赖所使用的算法，不过你至少可以约束决策树的最大深度。在 Scikit-Learn 中，它是由`max_depth`超参数控制的（默认值为`None`，意味着没有限制）。减少`max_depth`可以正则化模型，继而减少过拟合的风险。

`DecisionTreeClassifier`类还有其他能约束决策树形状的参数：`min_samples_split`（节点被划分之前必须有的样本最小数量），`min_samples_leaf`（叶节点必须有的样本最小数量），`min_weight_fraction_leaf`（和`min_samples_leaf`一样，但表示为加权实例总数的分数），`max_leaf_nodes`（叶节点的最大数量），`max_features`（每个节点划分时被评估的特征的最大数量）。增加`min_*`超参数或减少`max_*`超参数都可以正则化模型。

> **笔记**
> 其他的算法工作时，先训练一个不加约束的决策树，然后对不必要的节点进行**剪枝**（*pruning*）（删除）。如果一个节点的子节点全为叶子节点，对纯度的提升没有统计学的意义，则被认为是不必要的。
>
> 标准的假设检验，比如卡方检验，用于估计改进纯粹是偶然结果（被称为**零假设**（*null hypothesis*））的概率。如果这个概率——被称为 p 值（*p-value*）——高于给出的阈值（通常是 5 %，由超参数控制），那么节点被认为是不必要的，它的子节点会被删除。剪枝过程会持续进行，直到所有不必要的节点都被删除为止。

图 6-3 展示了两棵在卫星数据集（在第五章介绍过）上训练的决策树。在左图中，决策树使用默认的超参数（即没有约束）进行训练，而在右图中决策树训练时`min_samples_leaf=4`。很显然，左图的模型过拟合了，而右图的模型泛化得就很好。

![3](./images/chap6/6-3.png)

## 回归

决策树也可以实现回归任务。让我们用 Sciki-Learn 的`DecisionTreeRegressor`类来创建一棵回归树，设置`max_depth=2`，在有噪音的二次数据集上训练它。

```python
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X,	y)
```

生成的树如图 6-4 所示。

![4](./images/chap6/6-4.png)

这棵树看起来和之前创建的分类树有点相似。主要的不同在于它预测具体的值，而不是每个节点的类别。例如，假设你想要对一个新实例 ![](http://latex.codecogs.com/gif.latex?x_1%20%3D0.6) 进行预测。你从根部开始遍历整棵树，最终到达的叶节点预测`value=0.1106`。这个预测仅仅是与该叶节点相关的 110 个训练实例的平均目标值。该预测结果在 110 个实例上的 均方根误差（ MSE ）等于 0.0151。

图 6-5 的左图显示的是该模型的预测结果。如果你设置`max_depth=3`，你就会得到右图的预测。注意每一区域的预测值总是该区域实例目标值的平均值。算法划分每个区域的方法是使大多数的训练实例尽可能靠近预测值。

![5](./images/chap6/6-5.png)

CART 算法工作的方法和之前几乎一样，除了它现在划分训练集是以一种能最小化均方根误差的方法，而不是最小化不纯度的方法。公式 6-4 显示了算法要最小化的损失函数。

![](http://latex.codecogs.com/gif.latex?J%28k%2Ci_k%29%3D%5Cfrac%7Bm_%7B%5Cmathrm%7Bleft%7D%7D%7D%7Bm%7D%5Cmathrm%7BMSE%7D_%7B%5Cmathrm%7Bleft%7D%7D&plus;%5Cfrac%7Bm_%7B%5Cmathrm%7Bright%7D%7D%7D%7Bm%7D%5Cmathrm%7BMSE%7D_%7B%5Cmathrm%7Bright%7D%7D%5C%3B%5C%3B%5C%3B%5Cmathrm%7Bwhere%7D%5Cbegin%7Bcases%7D%20%5Cmathrm%7BMSE%7D_%7B%5Cmathrm%7Bnode%7D%7D%3D%5Csum_%7Bi%5Cin%20%5Cmathrm%7Bnode%7D%7D%28%5Chat%7By%7D_%7B%5Cmathrm%7Bnode%7D%7D-y%5E%7B%28i%29%7D%29%5E2%5C%5C%20%5Chat%7By%7D_%7B%5Cmathrm%7Bnode%7D%7D%3D%5Cfrac%7B1%7D%7Bm_%7B%5Cmathrm%7Bnode%7D%7D%7D%5Csum_%7Bi%5Cin%20%5Cmathrm%7Bnode%7D%7Dy%5E%7B%28i%29%7D%20%5Cend%7Bcases%7D)

就像分类任务一样，决策树在处理回归任务时也容易过拟合。如果不进行正则化（即使用默认的超参数），你会得到图 6-6 左图中的预测结果。它显然严重过拟合了训练数据。设置`min_samples_leaf=10`能得到一个更合理的模型，见图 6-6 的右图。

![6](./images/chap6/6-6.png)

## 不稳定性

希望你现在能了解决策树有许多特点：它们易于理解和说明，易于使用，多功能且强大。不过它们也有一些限制。首先，你可能已经注意到了，决策树很喜欢正交的决策边界（所有的划分都与坐标轴垂直），这使它们对训练集的旋转很敏感。例如，图 6-7 展示了一个简单的线性可分数据集：在左图，决策树能轻易划分它们。而在右图，在数据旋转 45° 后，决策边界变得不必要地复杂了。尽管两种决策树都完美拟合了训练集，右边的可能无法泛化得很好。一种能限制这个问题的方法是使用 PCA （见第八章），能使训练数据变好一些。

![7](./images/chap6/6-7.png)

更普遍地，决策树主要的问题是它们对训练数据中的微小变化很敏感。例如，如果你只是从鸢尾花数据中移动最宽的变色鸢尾（花瓣长 4.8 厘米，宽 1.8 厘米），并训练一棵新的决策树，你可能会得到图 6-8 中的模型。如你所见，它和先前的决策树（图 6-2 ）差别很大。事实上，因为 Scikit-Learn 使用的训练算法是随机的，即使是在相同的训练集上也可能得到不同的模型（除非你设置`random_state`超参数）。

![8](./images/chap6/6-8.png)

## 练习

1. 在 100 万实例的数据上训练出的决策树（不加正则）深度大约为多少？
2. 一个节点的基尼不纯度通常是低于还是高于它的父节点？它是通常更低/更高，还是总是更低/更高？
3. 如果决策树过拟合数据了，减少`max_depth`是个好主意吗？
4. 如果决策树欠拟合数据了，对输入特征进行缩放是个好主意吗？
5. 如果在包含 100 万实例的训练集上训练决策树需要花费一小时，那么在 1000 万实例的数据集上训练大约要花多少时间？
6. 如果你的训练集包含 10 万实例，设置`presort=True`能加快训练速度吗？
7. 在卫星数据集上训练并微调决策树。
	- 使用`make_moons(n_samples=10000, noise=0.4)`来生成卫星数据集。
	- 使用`train_test_split()`将其划分为训练集和测试集。
	- 进行交叉验证，使用网格搜索（`GridSearchCV`类）来找到`DecisionTreeClassifier`的最佳超参数。提示：在`max_leaf_nodes`上尝试多种值。
	- 在完整数据集上用这些超参数训练模型，在测试集上测量模型的性能。你大概能得到 85% - 87% 的精度。
8. 生成森林。
	- 继续上一个练习，生成训练集的 1000 个子集，每个子集都包含随机选中的 100 个实例。提示：可以使用 Scikit-Learn 的`ShuffleSplit`类。
	- 在每个子集上各训练一棵决策树，使用之前发现的最佳超参数。在测试集上评估 1000 棵决策树。因为它们是在更小的集合上训练的，很可能比第一棵决策树的性能要差，只有 80% 的精度。
	- 现在是见证奇迹的时刻。对于每个测试集实例，生成 1000 棵决策树的预测，只保留出现最频繁的预测（可以使用 SciPy 的`mode()`函数）。这样就能得到在测试集上的少数服从多数预测（*majority-vote predictions*）。
	- 在测试集上评估这些预测：你应该会得到比初始模型稍高一些的精度（大约高 0.5 % 到 1.5 %）。恭喜，你训练出了一个随机森林分类器！
